{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction to Pytorch Aggregation by way of NumPy\n",
    "\n",
    "![Assets/NumpyAxis0.PNG](Assets/NumpyAxis0.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# Exercises:\n",
    "\n",
    "Do a page search for each **Exercise** in this notebook. Complete all exercises. Code in cells above each exercise may give insight into a solid approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from math import log10 as lg10\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whatever loopy code you have - spend time looking for alternatives such as this. The acceleration can be exrardinary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Numpy Aggregation\n",
    "\n",
    "Aggregation is where we operate on an array and generate resulting data with a smaller dimension than the original array\n",
    "\n",
    "The aggregations can typically be done using different axes to control the direction\n",
    "\n",
    "![Assets/NumpyAxis0.PNG](Assets/NumpyAxis0.PNG)\n",
    "\n",
    "![Assets/NumpyAxis11.PNG](Assets/NumpyAxis1.PNG)\n",
    "\n",
    "Common examples in AI are:\n",
    "- min\n",
    "- max\n",
    "- sum\n",
    "- mean\n",
    "- std ... among others\n",
    "\n",
    "----------------------------------------------------------------------------------\n",
    "| Functions | Description | \n",
    "| --- | --- |\n",
    "| np.mean() | Compute the arithmetic mean along the specified axis. |\n",
    "| np.std() | Compute the standard deviation along the specified axis. |\n",
    "| np.var() | Compute the variance along the specified axis. |\n",
    "| np.sum() | Sum of array elements over a given axis. |\n",
    "| np.prod() | Return the product of array elements over a given axis. |\n",
    "| np.cumsum() | Return the cumulative sum of the elements along a given axis. |\n",
    "| np.cumprod() | Return the cumulative product of elements along a given axis. |\n",
    "| np.min(), np.max() | Return the minimum / maximum of an array or minimum along an axis. |\n",
    "| np.argmin(), np.argmax() | Returns the indices of the minimum / maximum values along an axis |\n",
    "| np.all() | Test whether all array elements along a given axis evaluate to True. |\n",
    "| np.any() | Test whether any array element along a given axis evaluates to True. |\n",
    "\n",
    "\n",
    "Specialty calcualtions exist so always eamine your code with a view to simply and remove loops with off the shelf solutions\n",
    "\n",
    "For example, in AI there re times we need to add the values of the diagonal of special arrays.\n",
    "\n",
    "For very long vectors these will accelerate noticibly and more so for larger multdimensional arrays\n",
    "\n",
    "Below is a very partial list comparing PyTorch and NumPy aggregations:\n",
    "\n",
    "|  PyTorch Aggregations | NumPy Aggregations | \n",
    "| ---| --- | \n",
    "| torch.sum(x) | np.sum(x)| \n",
    "| torch.mean(x) | np.mean(x)| \n",
    "| torch.median(x) | np.median(x)| \n",
    "| torch.max(x) | np.max(x)| \n",
    "| torch.min(x) | np.min(x)| \n",
    "| torch.prod(x) | np.prod(x)| \n",
    "| torch.std(x) | np.std(x)| \n",
    "| torch.var(x) | np.var(x)| \n",
    "| torch.any(x) | np.any(x)| \n",
    "| torch.all(x) | np.all(x)| \n",
    "| torch.unique(x) | np.unique(x)| \n",
    "| torch.cumsum(x, dim=0) | np.cumsum(x) |\n",
    "\n",
    "\n",
    "Here are more comparison of usefule NumPy functions and their PyTorch counterparts:\n",
    "\n",
    "| Function | Numpy | PyTorch | \n",
    "| Test for NaN values| numpy.isnan()| torch.isnan()| \n",
    "| Test for infinite values| numpy.isinf()| torch.isinf()| \n",
    "| Test for negative infinite values| numpy.isneginf()| torch.isneginf()| \n",
    "| Test for positive infinite values| numpy.isposinf()| torch.isposinf()| \n",
    "| Test for finite values| numpy.isfinite()| torch.isfinite()| \n",
    "\n",
    "Below is a very partial list comparing PyTorch and NumPy aggregations:\n",
    "\n",
    "Below is a naive approach for addng all the diagnoal elements of a smallish array of 1000 x 1000. So the accerlation is reasonable but not outlandish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.arange(1_000_000).reshape(1000, 1000)\n",
    "A_torch = torch.tensor(A)\n",
    "Diag = 0\n",
    "\n",
    "t1 = time.time()\n",
    "for i in range(len(A)):\n",
    "    for j in range(len(A)): \n",
    "        if i == j:\n",
    "            Diag += A[i,j]\n",
    "t2 = time.time()\n",
    "Elapsed_Diag_base = t2-t1\n",
    "print(\"elapsed time: \", Elapsed_Diag_base)\n",
    "print(\"Diag: \", Diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "\n",
    "Use a search engine to find numpy method to find the sum of the diagonals of this array.\n",
    "- Hint: trace\n",
    "- Hint: Diag = np.trace(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "### Complete the code below #####\n",
    "\n",
    "Diag = np.trace(A)\n",
    "\n",
    "#####################   \n",
    "t2 = time.time()\n",
    "Elapsed_Diag_numpy = t2 - t1\n",
    "print(\"elapsed time: \", Elapsed_Diag_numpy)\n",
    "print(\"Diag: \", Diag)\n",
    "print(\"Acceleration: {:4.0f}X\".format(Elapsed_Diag_base/Elapsed_Diag_numpy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "### Complete the code below #####\n",
    "\n",
    "Diag = torch.trace(A_torch)\n",
    "\n",
    "#####################   \n",
    "t2 = time.time()\n",
    "Elapsed_Diag_numpy = t2 - t1\n",
    "print(\"elapsed time: \", Elapsed_Diag_numpy)\n",
    "print(\"Diag: \", Diag)\n",
    "print(\"Acceleration: {:4.0f}X\".format(Elapsed_Diag_base/Elapsed_Diag_numpy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Compute Mean & Std of array using NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(2021)\n",
    "# random.default_range is the recommended method for generated random's\n",
    "# see blog \"Stop using numpy.random.seed()\" for reasoning\n",
    "# https://towardsdatascience.com/stop-using-numpy-random-seed-581a9972805f\n",
    "\n",
    "a = rng.random((10_000_000,))\n",
    "a_torch = torch.tensor(a)\n",
    "t1 = time.time()\n",
    "timing = {}\n",
    "S = 0\n",
    "for i in range (len(a)):\n",
    "    S += a[i]\n",
    "mean = S/len(a)\n",
    "std = 0\n",
    "for i in range (len(a)):\n",
    "    d = a[i] - mean\n",
    "    std += d*d\n",
    "std = np.sqrt(std/len(a))\n",
    "timing['loop'] = time.time() - t1\n",
    "print(\"mean\", mean)\n",
    "print(\"std\", std)\n",
    "\n",
    "print(timing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "print(a.mean())\n",
    "print(a.std())\n",
    "\n",
    "timing['numpy'] = time.time() - t1\n",
    "print(timing)\n",
    "print(f\"Acceleration {timing['loop']/timing['numpy']:4.1f} X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "print(a_torch.mean())\n",
    "print(a_torch.std())\n",
    "\n",
    "timing['pytorch'] = time.time() - t1\n",
    "print(timing)\n",
    "print(f\"Acceleration {timing['loop']/timing['numpy']:4.1f} X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"Measure acceleration of looping versus Numpy log10 [Lower is better]\",fontsize=12)\n",
    "plt.ylabel(\"Time in seconds\",fontsize=12)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Various types of operations\",fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.bar(x = list(timing.keys()), height= list(timing.values()), align='center',tick_label=list(timing.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notices and Disclaimers\n",
    "\n",
    "Intel technologies may require enabled hardware, software or service activation.\n",
    "No product or component can be absolutely secure. \n",
    "\n",
    "Your costs and results may vary. \n",
    "\n",
    "Â© Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
